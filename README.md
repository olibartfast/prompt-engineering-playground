Sandbox for experimenting with LLM(Large Languange Models) LMM(Large Multimodal Models) and VLM (Vision Language Model)

### OpenAI API-compliant C++ multimodal inference client:
* [C++ code](OpenAI-completion-client/cpp/Readme.md)
* [Python code](OpenAI-completion-client/python/Readme.md) (TODO Image Resize in python code)

### Some benchmarks sites
* https://mmmu-benchmark.github.io/
* https://lmarena.ai/
* https://huggingface.co/spaces/lmsys/chatbot-arena-leaderboard
* https://huggingface.co/spaces/opencompass/openvlm_video_leaderboard

### Online courses
* https://www.deeplearning.ai/short-courses/prompt-engineering-for-vision-models/
* https://www.deeplearning.ai/short-courses/large-multimodal-model-prompting-with-gemini/

### Vision Multimodal API
https://docs.anthropic.com/en/docs/build-with-claude/vision
https://platform.openai.com/docs/guides/vision
https://cloud.google.com/vision?hl=en
https://azure.microsoft.com/en-us/products/ai-services/ai-vision
https://aws.amazon.com/rekognition/
