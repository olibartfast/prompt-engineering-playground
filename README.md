Sandbox for experimenting with LLM(Large Languange Models) LMM(Large Multimodal Models) and VLM (Vision Language Model)

### Some benchmarks sites
* https://mmmu-benchmark.github.io
* https://lmarena.ai
* https://rank.opencompass.org.cn/home

### Online courses
* https://www.deeplearning.ai/short-courses/prompt-engineering-for-vision-models/
* https://www.deeplearning.ai/short-courses/large-multimodal-model-prompting-with-gemini/

### Vision Multimodal API Services (tentative list)
* https://docs.anthropic.com/en/docs/build-with-claude/vision
* https://platform.openai.com/docs/guides/vision
* https://cloud.google.com/vision?hl=en
* https://azure.microsoft.com/en-us/products/ai-services/ai-vision
* https://aws.amazon.com/rekognition/
* https://platform.stability.ai/docs/api-reference

## Inference
  ### OpenAI API-compliant C++ multimodal inference client:
  * [C++ code](OpenAI-completion-client/cpp/Readme.md)
  * [Python code](OpenAI-completion-client/python/Readme.md) (TODO Image Resize in python code)

  ### TensorRT-LLM inference client python/c++
  * TODO check documentation:
     * https://github.com/NVIDIA/TensorRT-LLM/blob/main/examples/multimodal/README.md
     * https://github.com/NVIDIA/TensorRT-LLM/blob/main/examples/cpp/executor/README.md
